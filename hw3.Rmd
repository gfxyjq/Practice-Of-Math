---
title: "Homework 3"
author: "3230104912 Zhai Guoqing"
date: \today
output:
  pdf_document:
    latex_engine: xelatex
  html_document:
    df_print: paged
    
---

```{r setup, message = F, include=FALSE}
knitr::opts_chunk$set(warning = FALSE)
options(htmltools.dir.version = FALSE)
library(tidyverse)
library(DAAG)
```

```{r}
gmp <- read.table("data/gmp.dat")
gmp$pop <- round(gmp$gmp/gmp$pcgmp)
```
# 1.Plotting the data
```{r}
plot(gmp$pop, gmp$pcgmp, log = "xy",
     xlab = "Population (log)", ylab = "Per Capita GMP (log)",
     main = "City Scaling Relationship", pch = 16, col = "gray")

curve(6611 * x^(1/8), add = TRUE, col = "blue", lwd = 2)
curve(6611 * x^0.10, add = TRUE, col = "red", lwd = 2)
curve(6611 * x^0.15, add = TRUE, col = "green", lwd = 2)

legend("topright",
       legend = c("a=0.125(default)", "a=0.10", "a=0.15"),
       col = c("blue", "red", "green"), lwd = 2)

grid(col = "lightgray")
```

# 2.The function mse()
```{r}
mse <- function(params, N = gmp$pop, Y = gmp$pcgmp) {
  y0 <- params[1]
  a <- params[2]
  if (any(is.na(c(y0, a)))) {
    return(Inf)
  }
  if (y0 <= 0 || a < 0) {
    return(Inf)
  }
  Y_pred <- y0 * (N^a)
  mean((Y - Y_pred)^2, na.rm = TRUE)
}
mse(c(6611, 0.15)) #expected result:207057513
mse(c(5000, 0.10)) #expected result:298459914
```
# 3.Built-in functions for optimization
```{r}
fit1 <- nlm(mse, c(y0=6611, a=1/8))
fit2 <- nlm(mse, c(y0=5000, a=0.10))
fit3 <- nlm(mse, c(y0=8000, a=0.15))

results <- data.frame(
  Initial = c("y0=6611, a=0.125", "y0=5000, a=0.10", "y0=8000, a=0.15"),
  Minimum = c(fit1$minimum, fit2$minimum, fit3$minimum),
  y0_est = c(fit1$estimate[1], fit2$estimate[1], fit3$estimate[1]),
  a_est = c(fit1$estimate[2], fit2$estimate[2], fit3$estimate[2])
)
print(results)
```

Minimum represents the lowest Mean Squared Error (MSE) achieved during optimization. It quantifies how well the model fits the data (smaller = better fit).

Estimate is a vector of the optimized parameters (y0 and a) that minimize the MSE:

estimate[1] (y0): Baseline productivity, theoretical per capita GMP for a city of population 1.

estimate[2] (a): Scaling exponent, a 1% increase in population boosts per capita GMP by a.

# 4.The function plm()
```{r}
plm <- function(y0_init, a_init, N = gmp$pop, Y = gmp$pcgmp) {
  fit <- nlm(
    f = mse,
    p = c(y0_init, a_init),
    N = N,
    Y = Y,
    iterlim = 1000,
    steptol = 1e-10
  )
  
  list(
    y0 = fit$estimate[1],
    a = fit$estimate[2],
    mse = fit$minimum
  )
}

result1 <- plm(y0_init = 6611, a_init = 0.15)
print(result1)
```

```{r}
result2 <- plm(y0_init = 5000, a_init = 0.10)
print(result2)
```

The optimizer found slightly different solutions because it started from different points. Real data isn't perfect, so small variations happen.

Result1 (y₀=6611, a=0.126) has the lower MSE (61.8M vs 62.5M), meaning it fits the data slightly better.

# 5.Convincement of jackknife
## a.The mean and the standard error
```{r}
mean_pcgmp <- mean(gmp$pcgmp)
sd_pcgmp <- sd(gmp$pcgmp)
n <- length(gmp$pcgmp)
sem_pcgmp <- sd_pcgmp / sqrt(n)

cat("Mean per-capita GMP:", round(mean_pcgmp, 2), "USD\n")
cat("Standard Deviation:", round(sd_pcgmp, 2), "USD\n")
cat("Standard Error of the Mean:", round(sem_pcgmp, 2), "USD\n")
cat("Number of cities:", n, "\n")
```

## b.The function leave_one_out_mean()
```{r}
leave_one_out_mean <- function(i, pcgmp_vector = gmp$pcgmp) {
  mean(pcgmp_vector[-i])
}
```

## c.The vector jackknifed.means
```{r}
jackknifed.means <- sapply(1:length(gmp$pcgmp), leave_one_out_mean)
```

## d.the jack-knife approximation to the standard error
```{r}
jackknife_se <- sqrt(
  ((n - 1) / n) * 
  sum((jackknifed.means - mean(jackknifed.means))^2)
)

cat("the jack-knife approximation to the standard error of the mean:", jackknife_se, "\n")
```
Observation shows that the data matches the results of section a.

# 6.The function plm.jackknife()
```{r}
plm.jackknife <- function(y0_init, a_init, N = gmp$pop, Y = gmp$pcgmp) {
  n <- length(N)
  
  leave_one_out_plm <- function(i) {
    fit <- plm(y0_init, a_init, N[-i], Y[-i])
    c(y0 = fit$y0, a = fit$a)
  }
  jackknifed.params <- sapply(1:n, leave_one_out_plm)
  
  se.y0 <- sqrt(((n-1)/n) * sum((jackknifed.params["y0",] - mean(jackknifed.params["y0",]))^2))
  se.a <- sqrt(((n-1)/n) * sum((jackknifed.params["a",] - mean(jackknifed.params["a",]))^2))
  
  list(se.y0 = se.y0, se.a = se.a)
}

result <- plm.jackknife(y0_init = 6611, a_init = 0.15)
cat("The standard error of y0:", result$se.y0, "\n",
    "The standard error of a:", result$se.a, "\n")
```

# 7.Data gmp-2013
```{r}
gmp2013 <- read.table('data/gmp-2013.dat', header = T)
gmp2013$pop <- round(gmp2013$gmp/gmp2013$pcgmp)
fit2013 <- plm(y0_init = 6611, a_init = 0.15, 
               N = gmp2013$pop, Y = gmp2013$pcgmp)

jack2013 <- plm.jackknife(y0_init = 6611, a_init = 0.15,
                         N = gmp2013$pop, Y = gmp2013$pcgmp)

cat("2013 parameter estimation:\n",
    "y0 =", fit2013$y0, "±", jack2013$se.y0, "\n",
    "a =", fit2013$a, "±", jack2013$se.a, "\n")
```
Neither parameter (y₀ or a) changed significantly from their initial values, as the differences were either zero (y₀) or within the margin of error (a), with large standard errors indicating high uncertainty in the estimates.